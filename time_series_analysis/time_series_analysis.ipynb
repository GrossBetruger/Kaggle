{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5ea01-f22c-456e-b10f-919dfef0b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b224fc5-eaac-47e4-b630-5cb1c09e7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path('data') / 'ice_cream.csv')\n",
    "data.rename(columns={'DATE': 'date', 'IPN31152N': 'production'}, inplace=True)\n",
    "data.date = pd.to_datetime(data.date)\n",
    "data.set_index('date', inplace=True)\n",
    "start_date = pd.to_datetime('2010-01-01')\n",
    "data = data[start_date:]\n",
    "\n",
    "plt.title('Ice Cream Production Year by Year')\n",
    "plt.ylabel('production')\n",
    "plt.xlabel('year')\n",
    "for year in range(2010, 2021):\n",
    "    plt.axvline(pd.to_datetime(str(year) + '-01-01'), \n",
    "                color='black',\n",
    "                linestyle='--')\n",
    "plt.plot(data.production)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1470f-abb8-43f9-a621-da735de8f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "\n",
    "# acf = plot_acf(data.production, lags=120) # 12 months * 10 years = 120 lags\n",
    "autocorrelation_plot(data.production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94fa10-0551-4de6-8e1f-2bd5184a7432",
   "metadata": {},
   "source": [
    "# Based on decaying ACF we're probably dealing with an autoregressive process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f602c-c8ac-4ea7-8ae4-368b2b537d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf = plot_pacf(data.production, method='ywm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00d829-0620-4208-89b0-b73f567ef26f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Based on PACF we should start with an autoregressive model on lags 1, 2, 3, 10, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fbb3c-d20e-42bb-a1c1-795be5d26659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = data.index.values\n",
    "# y = data.production.values\n",
    "\n",
    "train, test = data[:60].values, data[60:].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597292e3-f358-4b51-8761-6ddc1aeb9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = AutoReg(train, lags=[1, 2, 3, 10, 13])\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "print('Coefficients: %s' % model_fit.params)\n",
    "\n",
    "plt.plot(predictions, label='predictions')\n",
    "plt.plot(test, label='true values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "f'MSE: {mean_squared_error(predictions, test)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ba8fd-292c-477e-ba10-bab27bd2320d",
   "metadata": {},
   "source": [
    "## now let's do the same thing assuming 12-months seasonality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee54fcf-6f47-4a91-9265-3f52f637e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoReg(train, lags=[1, 2, 3, 10, 13], seasonal=True, period=12)\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "# print('Coefficients: %s' % len(model_fit.params))\n",
    "\n",
    "plt.plot(predictions, label='predictions')\n",
    "plt.plot(test, label='true values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "f'MSE: {mean_squared_error(predictions, test)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2444a05-651b-4fb6-8053-80ddc9009bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bingo... looks better and the MSE is way lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9126a11-0cd1-4867-bda8-c594806d674f",
   "metadata": {},
   "source": [
    "### Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc88711-972a-4963-81da-13ff807bf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# augmented dickey-fuller (for AR models more comlex than AR1)\n",
    "\n",
    "adf_test_value, p_value, used_lags, nobs, _, _ = adfuller(data['production'].values)\n",
    "print(f'''adf statistic: {adf_test_value} (lower than zero means no unit-roots)\n",
    "p value: {p_value}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0019278-a0e1-4df3-8c5e-09561b6fec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to retrain the model with the number of lags produced by the augmented dickey-fuller\n",
    "# test\n",
    "\n",
    "model = AutoReg(train, lags=used_lags, seasonal=True, period=12)\n",
    "model_fit = model.fit()\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "# print('Coefficients: %s' % len(model_fit.params))\n",
    "\n",
    "plt.plot(predictions, label='predictions')\n",
    "plt.plot(test, label='true values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'''MSE: {mean_squared_error(predictions, test)}\n",
    "using \"number of lags\" from the aug-dickey-fuller test helped!''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0019549-73a7-4512-a9cd-6a628911281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ar_process(lags: int, coefs: list[float], length: int) -> np.array:\n",
    "    if len(coefs) != lags:\n",
    "        raise ValueError(f'number of coefs ({len(coefs)}) must match lags: ({lags})')\n",
    "        \n",
    "    coefs = np.array(coefs)\n",
    "    series = [np.random.normal() for _ in range(lags)] # generate first lags (normally distributed)\n",
    "    \n",
    "    for _ in range(length - lags):\n",
    "        previous_values = series[-lags:][::-1] # take 'lags' values from in reversed order\n",
    "        new_value = np.sum(np.array(previous_values) * coefs + np.random.normal())\n",
    "        series.append(new_value)\n",
    "    return np.array(series)\n",
    "\n",
    "lags = 3\n",
    "for coef in [0.1, 0.2, 0.33333, 0.334, 0.5]:\n",
    "    coefs = [coef, coef, coef]\n",
    "    generated_tseries = generate_ar_process(lags, coefs, 100)\n",
    "    plt.title(f'''AR process with {lags} lags and coefs sum to: {sum(coefs)}\n",
    "    adf statistic: {adfuller(generated_tseries)[0]}\n",
    "    p value: {adfuller(generated_tseries)[1]}''')\n",
    "    plt.plot(generated_tseries)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9882c1-362b-4573-9d5e-2fa0d2bedb0f",
   "metadata": {},
   "source": [
    "### process remains stationary as long as sum of coefs is lower than 1\n",
    "### when it's 1 and higher it gets a clean trend (trend-stationary?) \n",
    "### when it's significantly higher than 1 it become a smooth exponential "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537ffe0-c231-4540-97d5-5b07eaefb0af",
   "metadata": {},
   "source": [
    "# S&P500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999284ff-fc9d-4156-9de3-5ede7d607db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance \n",
    "\n",
    "spy_data = yfinance.Ticker('SPY')\n",
    "spy_data = spy_data.history(period='1d', start='2010-01-01', end='2020-01-01')\n",
    "closing_prices = spy_data[['Close']]\n",
    "plt.title('SPY daily closing price')\n",
    "plt.plot(closing_prices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb25590-6f69-49e3-b6b9-a9148e5064b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_diffs = closing_prices.values[1:] - closing_prices[:-1]\n",
    "plt.title('SPY daily movements')\n",
    "plt.ylabel('Price difference')\n",
    "plt.plot(daily_diffs)\n",
    "for year in range(2010, 2021):\n",
    "    plt.axvline(pd.to_datetime(str(year) + '-01-01'), linestyle='--', color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fcddb-e446-422e-ae21-6477d9e7d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_plot = plot_acf(daily_diffs, lags=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce9a85-e965-47c8-8bdf-fe5363323981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_plot = plot_pacf(daily_diffs, method='ywm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1107c9-5fc1-4543-8c9f-82b72e2f7354",
   "metadata": {},
   "source": [
    "# no significant autocorrelation (predict stock-market movements is not easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a174e-b45f-418a-9bd2-8916e9b6a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker: str, start_date: str = '1920-01-01', end_date: str = '2022-01-01', period='1d'):\n",
    "    ticker_data = yfinance.Ticker(ticker)\n",
    "    ticker_data = ticker_data.history(period='1d', start=start_date, end=end_date)\n",
    "    if len(ticker_data) == 0:\n",
    "        raise Exception(f'no data for: {ticker}')\n",
    "    return ticker_data[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67514b50-6a43-4c5e-b412-bd3be0b53084",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data = pd.DataFrame()\n",
    "start, end = '2013-11-06','2021-01-01'\n",
    "market_data['SPY'] = get_ticker_data('SPY', start_date=start, end_date=end)\n",
    "market_data['DAX'] = get_ticker_data('DAX', start_date=start, end_date=end)\n",
    "sse_index_tracking_fund = 'ASHR' #Xtrackers Hvst CSI 300 China A-Shs ETF\n",
    "market_data[sse_index_tracking_fund] = get_ticker_data(sse_index_tracking_fund, start_date=start, end_date=end)\n",
    "market_data.plot() # market_data.plot() and not plt.plot(market_data) to get labels automatically from cols\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff3be1-a3c9-4c2a-9e9e-4ba612ce9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data.corr() # correlation between markets seems between ~62% end ~66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cfc29-9f27-43b8-99dd-dabef4ba56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def clean_mutual_nans(df: pd.DataFrame, col1: str, col2: str):\n",
    "    return df[df[col2].notnull()][col1].dropna(), df[df[col1].notnull()][col2].dropna()\n",
    "\n",
    "ashr, dax = clean_mutual_nans(market_data, 'ASHR', 'DAX')\n",
    "r, p_value = stats.pearsonr(ashr, dax)\n",
    "print('pearson r:', r, 'p value: ', p_value) \n",
    "\n",
    "r, p_value = stats.pearsonr(market_data.interpolate(method='time').fillna(method='bfill')['ASHR'],\n",
    "                            market_data.interpolate(method='time').fillna(method='bfill')['DAX'])\n",
    "print('(interpolated data) pearson r:', r, 'p value: ', p_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349c9ba-3d87-41c5-b38f-673f41f010bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot moving averages\n",
    "tsla_data = pd.DataFrame()\n",
    "tsla_data['price'] = get_ticker_data('TSLA', start_date='2010-01-01', end_date='2022-01-01')\n",
    "tsla_data['100d-MA'] = tsla_data['price'].rolling(100).mean()\n",
    "tsla_data['200d-MA'] = tsla_data['price'].rolling(200).mean()\n",
    "tsla_data.dropna()[-100:].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130b7dd-d3b4-421e-9a3a-c1320f1efe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_data = get_ticker_data('VIX', start_date='2015-01-01', end_date='2018-01-01').apply(lambda x: x / 100)\n",
    "volatility_data['SPY'] = get_ticker_data('SPY', start_date='2015-01-01', end_date='2018-01-01')\n",
    "volatility_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49989875-13ea-4bf7-9345-1acfae5e308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roi(stock_price: pd.DataFrame):\n",
    "    return stock_price.values[-1] / stock_price.values[0]\n",
    "\n",
    "spy = get_ticker_data('SPY')\n",
    "calc_roi(spy[pd.to_datetime('2020-12-01'):pd.to_datetime('2021-09-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04b40-4980-452c-92b8-13e5d4e9564b",
   "metadata": {},
   "source": [
    "## search for TLCC: Time Lagged Cross Correlation â€” assessing signal dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5b052-0c71-4a5b-9900-0c10ecc59f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy, ashr = clean_mutual_nans(market_data, 'SPY', 'ASHR')\n",
    "corr_df = pd.DataFrame()\n",
    "for shift_periods in [10, 20, 30, 50]:\n",
    "# shift_periods = 30\n",
    "    corr_df['spy'] = spy\n",
    "    corr_df['spy_shift'] = spy.shift(periods=shift_periods, freq='1D')\n",
    "    corr_df['ashr'] = ashr\n",
    "    corr_df['ashr_shift'] = ashr.shift(periods=shift_periods, freq='1D')\n",
    "    assert corr_df.corr()['spy']['ashr_shift'] < corr_df.corr()['spy']['ashr']\n",
    "    assert corr_df.corr()['ashr']['spy_shift'] < corr_df.corr()['ashr']['spy']\n",
    "# doesn't look like there's some kind of TLCC...\n",
    "corr_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf05a9e-82cd-48be-8a09-aac60f3cc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert corr_df.corr()['spy']['ashr_shift'] < corr_df.corr()['spy_shift']['ashr']\n",
    "# also looks like more information is flowing from ashr to spy than the other way round "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
